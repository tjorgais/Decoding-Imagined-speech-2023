{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = {\"Long_words\": \"/home/tseringj/final_project/Long_Words\",\n",
    "               \"Short_Long_words\": \"/home/tseringj/final_project/Short_Long_words\",\n",
    "               \"Short_words\": \"/home/tseringj/final_project/Short_words\",\n",
    "               \"Vowels\": \"/home/tseringj/final_project/Vowels\"}\n",
    "\n",
    "words_dict = {\n",
    "    \"Long_words\": [\"cooperate\", \"independent\"],\n",
    "    \"Short_Long_words\": [\"cooperate\", \"in\"],\n",
    "    \"Short_words\": [\"out\", \"in\", \"up\"],\n",
    "    \"Vowels\": [\"a\", \"i\", \"u\"]\n",
    "}\n",
    "\n",
    "numeric_labels = {\n",
    "    \"Long_words\": {\"cooperate\": 0, \"independent\": 1},\n",
    "    \"Short_Long_words\": {\"cooperate\": 0, \"in\": 1},\n",
    "    \"Short_words\": {\"out\": 0, \"in\": 1, \"up\": 2},\n",
    "    \"Vowels\": {\"a\": 0, \"i\": 1, \"u\": 2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasub2 = sio.loadmat('Long_Words/sub_2b_ch64_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']\n",
    "datasub3 = sio.loadmat('Long_Words/sub_3b_ch80_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']\n",
    "datasub6 = sio.loadmat('Long_Words/sub_6_ch64_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']\n",
    "datasub7 = sio.loadmat('Long_Words/sub_7_ch64_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']\n",
    "datasub9 = sio.loadmat('Long_Words/sub_9c_ch64_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']\n",
    "datasub11 = sio.loadmat('Long_Words/sub_11b_ch64_l_eog_removed_256Hz.mat')['eeg_data_wrt_task_rep_no_eog_256Hz_last_beep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix_to_load = \"eeg_data_wrt_task_rep_no_eog_256Hz_last_beep\"\n",
    "\n",
    "def load_EEG(type, subject_no):\n",
    "    path = folder_path[type]\n",
    "    words = words_dict[type]\n",
    "    for subject_file in os.scandir(path):\n",
    "        if not (subject_file.is_file() and subject_file.name.endswith('.mat') and\n",
    "                int(re.search(\"[0-9]+\", subject_file.name).group(0)) == subject_no):\n",
    "            continue\n",
    "        mat = sio.loadmat(subject_file.path)[matrix_to_load]\n",
    "        \n",
    "        temp = f\"{path}/temp_files3\"\n",
    "        if not os.path.exists(temp):\n",
    "            os.mkdir(temp)\n",
    "        temp = f\"{temp}/{subject_no}\"\n",
    "\n",
    "        if not os.path.exists(temp):\n",
    "            os.mkdir(temp)\n",
    "        X = []\n",
    "        Y = []\n",
    "        for index, eeg in np.ndenumerate(mat):\n",
    "            temp2 = f\"{temp}/{words[index[0]]}_{index[1] + 1}.npy\" #storing each trial\n",
    "            X.append(temp2)\n",
    "            Y.append(words[index[0]])\n",
    "            if not os.path.exists(temp2):\n",
    "                np.save(temp2, eeg)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function for data augmentation\n",
    "\n",
    "# def train_augmentation(X,Y):\n",
    "\n",
    "#     final_X=np.empty((0,64,1280))\n",
    "#     label=np.empty((0,1))\n",
    "\n",
    "    \n",
    "#     for i in range(len(X)):\n",
    "#         #result=np.empty((4,64,512))\n",
    "#         with open(X[i], 'rb') as f:\n",
    "#             data = np.load(f)\n",
    "#             # indices = np.arange(1, 17)\n",
    "#             # indices = np.append(indices, np.arange(33, 49))\n",
    "#             # indices = np.delete(indices, 9)\n",
    "\n",
    "#             # Loop through the data with a stride of 64 samples\n",
    "\n",
    "            \n",
    "\n",
    "#             final_X = np.vstack((final_X,np.expand_dims(data,axis=0)))        \n",
    "#             if numeric_labels[type][Y[i]]==0:\n",
    "#                 label=np.vstack((label, np.zeros((1,1))))\n",
    "#             else:\n",
    "#                 label=np.vstack((label, np.ones((1,1))))\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "#     return final_X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data augmentation\n",
    "\n",
    "def train_augmentation(X,Y):\n",
    "\n",
    "    total_samples=1152\n",
    "    stride=192\n",
    "    epoch_size=512\n",
    "    final_X=np.empty((0,60,epoch_size))\n",
    "    label=np.empty((0,1))\n",
    "    print(f'with total_sample: {total_samples}, epoch size: {epoch_size} and strides: {stride}')\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        #result=np.empty((4,64,512))\n",
    "        with open(X[i], 'rb') as f:\n",
    "            data = np.load(f)\n",
    "            indices = np.arange(1, 32)\n",
    "            indices = np.append(indices, np.arange(33, 63))\n",
    "            indices = np.delete(indices, 8)\n",
    "            data=data[indices,:total_samples]\n",
    "            # Loop through the data with a stride of 64 samples\n",
    "            \n",
    "            for j in range(0, total_samples, stride):\n",
    "                if j+epoch_size >= total_samples:\n",
    "                    break\n",
    "            \n",
    "\n",
    "                final_X = np.vstack((final_X,np.expand_dims(data[:, j:j+epoch_size],axis=0)))        \n",
    "                if numeric_labels[type][Y[i]]==0:\n",
    "                    label=np.vstack((label, np.zeros((1,1))))\n",
    "                else:\n",
    "                    label=np.vstack((label, np.ones((1,1))))\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "    return final_X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # precision = precision_score(y_test, y_pred)\n",
    "    # recall = recall_score(y_test, y_pred)\n",
    "    # f1 = f1_score(y_test, y_pred)\n",
    "    #print(f'accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1 {f1}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "  # Import other classifiers as needed\n",
    "\n",
    "  # Train classifiers with different n_components values\n",
    "  model_accuracies={}\n",
    "  svm_rbf=SVC(kernel='rbf')\n",
    "\n",
    "  svm_rbf.fit(X_train, y_train)\n",
    "  y_pred_rbf=svm_rbf.predict(X_test)\n",
    "  accuracy_rbf=calculate_performance(y_test, y_pred_rbf)\n",
    "  model_accuracies['svm_rbf']=accuracy_rbf\n",
    "  \n",
    "  svm_linear = SVC(kernel='linear')\n",
    "  svm_linear.fit(X_train, y_train)\n",
    "  y_pred_linear = svm_linear.predict(X_test)\n",
    "  print(\"pca linear performance: \")\n",
    "  accuracy_linear=calculate_performance(y_test, y_pred_linear)\n",
    "  model_accuracies['svm_linear']=accuracy_linear\n",
    "\n",
    "  \n",
    "  svm_poly = SVC(kernel='poly')\n",
    "  svm_poly.fit(X_train, y_train)\n",
    "  y_pred_poly = svm_poly.predict(X_test)\n",
    "  print(\"pca linear performance: \")\n",
    "  accuracy_poly=calculate_performance(y_test, y_pred_poly)\n",
    "  model_accuracies['svm_poly']=accuracy_poly\n",
    "\n",
    "\n",
    "  \n",
    "  rfc = RandomForestClassifier()\n",
    "  rfc.fit(X_train, y_train)\n",
    "  y_pred_rfc = rfc.predict(X_test)\n",
    "  print(\"Random Forest performance: \")\n",
    "  accuracy_rfc=calculate_performance(y_test, y_pred_rfc)\n",
    "  model_accuracies['rfc']=accuracy_rfc\n",
    " \n",
    "  k = 5  \n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred_knn = knn.predict(X_test)\n",
    "  print(\"KNN: \")\n",
    "  accuracy_knn=calculate_performance(y_test, y_pred_knn)\n",
    "  model_accuracies['knn']=accuracy_knn\n",
    "\n",
    "\n",
    "  mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=2000,activation='relu', solver='adam', random_state=42)\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  y_pred_mlp = [round(value) for value in y_pred]\n",
    "  print('MLP performance: ')\n",
    "  accuracy_mlp=calculate_performance(y_test, y_pred_mlp)\n",
    "  model_accuracies['mlp']=accuracy_mlp\n",
    "\n",
    "\n",
    "  return model_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import LinAlgError\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_data(type,subject_no):\n",
    "    \n",
    "#     X,Y=load_EEG(type, subject_no)\n",
    "#     loo = LeaveOneOut()\n",
    "\n",
    "#     average_accuracies={}\n",
    "\n",
    "#     for i, (train_index, test_index) in enumerate(loo.split(X, Y)):\n",
    "#         train_X = X[train_index]\n",
    "#         train_y = Y[train_index]\n",
    "#         test_X = X[test_index]\n",
    "#         test_y = Y[test_index]\n",
    "#         X_train, y_train  = train_augmentation(train_X, train_y)\n",
    "#         X_test, y_test = train_augmentation(test_X, test_y)\n",
    "#         y_train=y_train.reshape((-1))\n",
    "#         y_test=y_test.reshape((-1))\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             csp = CSP(n_components=4, reg=0.0003, log=False, norm_trace=False)\n",
    "#             csp.fit(X_train, y_train)\n",
    "#             X_train_csp = csp.transform(X_train)\n",
    "#             X_test_csp = csp.transform(X_test)\n",
    "#         except LinAlgError:\n",
    "#             print(\"LinAlgError occurred. Adjusting regularization parameter...\")\n",
    "#             try:\n",
    "#                 csp = CSP(n_components=4, reg=0.0002, log=False, norm_trace=False)\n",
    "#                 csp.fit(X_train, y_train)\n",
    "#                 X_train_csp = csp.transform(X_train)\n",
    "#                 X_test_csp = csp.transform(X_test)\n",
    "#             except LinAlgError:\n",
    "#                 print(\"LinAlgError occurred again. Consider further adjustments or preprocessing steps.\")\n",
    "                \n",
    "#                 try:\n",
    "#                     csp = CSP(n_components=4, reg=0.0001, log=False, norm_trace=False)\n",
    "#                     csp.fit(X_train, y_train)\n",
    "#                     X_train_csp = csp.transform(X_train)\n",
    "#                     X_test_csp = csp.transform(X_test)\n",
    "#                 except LinAlgError:\n",
    "#                     print(\"LinAlgError occurred again. Consider further adjustments or preprocessing steps.\")\n",
    "#         print(f'fold {i} performance: ')\n",
    "#         accuracies=train_model(X_train_csp, X_test_csp, y_train, y_test)\n",
    "#         for model_name, accuracy in accuracies.items():\n",
    "#             if model_name not in average_accuracies:\n",
    "#                 average_accuracies[model_name] = []\n",
    "#             average_accuracies[model_name].append(accuracy)\n",
    "#     for model_name, accuracies in average_accuracies.items():\n",
    "#         average_accuracy = np.mean(accuracies)\n",
    "#         std_deviation = np.std(accuracies)\n",
    "#         print(f\"{model_name} Average Accuracy: {average_accuracy}\")\n",
    "#         print(f\"{model_name} Standard Deviation: {std_deviation}\")\n",
    "\n",
    "\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.classification import MDM, TSclassifier, FgMDM\n",
    "from pyriemann.estimation import Covariances\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pyriemann.channelselection import ElectrodeSelection\n",
    "\n",
    "from pyriemann.tangentspace import TangentSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(type,subject_no):\n",
    "    \n",
    "#     X,Y=load_EEG(type, subject_no)\n",
    "\n",
    "\n",
    "#     X_train, y_train  = train_augmentation(X, Y)\n",
    "\n",
    "#     y_train=y_train.reshape((-1))\n",
    "\n",
    "#     cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     cov_data_train = Covariances(estimator='lwf').transform(X_train)\n",
    "    \n",
    "#     mdm = MDM(metric=dict(mean='riemann', distance='riemann'))\n",
    "    \n",
    "\n",
    "#     scores_mdm = cross_val_score(mdm, cov_data_train, y_train, cv=cv, n_jobs=1)\n",
    "#     print(f\"MDM Classification Average Accuracy: {np.mean(scores_mdm)}, MDM Classification Average Std: {np.std(scores_mdm)}\")\n",
    "\n",
    "#     tsc = TSclassifier()\n",
    "#     # Use scikit-learn Pipeline with cross_val_score function\n",
    "#     scores_tsc = cross_val_score(tsc, cov_data_train, y_train, cv=cv, n_jobs=1)\n",
    "#     print(f\"TSC Classification Average Accuracy: {np.mean(scores_tsc)}, TSC Classification Average Std: {np.std(scores_tsc)}\")\n",
    "\n",
    "#     lr = LogisticRegression()\n",
    "#     csp = CSP(n_components=4, reg='ledoit_wolf', log=True)\n",
    "#     csp = Pipeline([('CSP', csp), ('LogisticRegression', lr)])\n",
    "#     scores_csp = cross_val_score(csp, X_train, y_train, cv=cv, n_jobs=1)\n",
    "#     print(f\" Classification Average Accuracy: {np.mean(scores_csp)}, MDM Classification Average Std: {np.std(scores_csp)}\")\n",
    "\n",
    "#     return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type=\"Long_words\"\n",
    "# subject_no=9\n",
    "# get_data(type, subject_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(type,subject_no):\n",
    "    \n",
    "#     X,Y=load_EEG(type, subject_no)\n",
    "#     kfold=5\n",
    "#     skf = StratifiedKFold(n_splits=kfold, random_state=42, shuffle=True)\n",
    "\n",
    "#     test_scores=[]\n",
    "\n",
    "    \n",
    "#     for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "#       train_X=X[train_index]\n",
    "#       train_y=Y[train_index]\n",
    "#       test_X=X[test_index]\n",
    "#       test_y=Y[test_index]\n",
    "\n",
    "#       X_train, y_train  = train_augmentation(train_X, train_y)\n",
    "#       X_test, y_test = train_augmentation(test_X, test_y)\n",
    "#       y_train=y_train.reshape((-1))\n",
    "#       y_test=y_test.reshape((-1))\n",
    "#       #cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "      \n",
    "#       cov_data_train = Covariances(estimator='lwf').transform(X_train)\n",
    "#       cov_data_test=Covariances(estimator='lwf').transform(X_test)\n",
    "#       ecs=ElectrodeSelection(nelec=30, metric='riemann', n_jobs=1)\n",
    "#       ecs.fit(cov_data_train, y_train)\n",
    "#       cov_data_train=ecs.transform(cov_data_train)\n",
    "#       cov_data_test=ecs.transform(cov_data_test)\n",
    "\n",
    "\n",
    "#       ts=TangentSpace(metric='riemann', tsupdate=True)\n",
    "#       data_train=ts.fit_transform(cov_data_train, y_train)\n",
    "#       data_test=ts.transform(cov_data_test)\n",
    "\n",
    "#       X_train_tensor = torch.Tensor(data_train)\n",
    "#       y_train_tensor = torch.Tensor(y_train)\n",
    "#       X_test_tensor = torch.Tensor(data_test)\n",
    "#       y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "#       # Create DataLoader for training and test sets\n",
    "#       train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#       train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#       test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "#       test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "#       num_classes=len(words_dict[type])\n",
    "\n",
    "#       # Define your feed-forward neural network\n",
    "#       class EEGClassifier(nn.Module):\n",
    "#           def __init__(self):\n",
    "#               super(EEGClassifier, self).__init__()\n",
    "#               self.fc1 = nn.Linear(in_features=data_train.shape[1], out_features=100)\n",
    "#               self.dropout1 = nn.Dropout(p=0.3)\n",
    "#               self.fc2 = nn.Linear(in_features=100, out_features=100)\n",
    "#               self.dropout2 = nn.Dropout(p=0.3)\n",
    "#               self.fc3 = nn.Linear(in_features=100, out_features=32)\n",
    "#               self.fc4 = nn.Linear(in_features=32, out_features=num_classes)\n",
    "\n",
    "#           def forward(self, x):\n",
    "#               x = torch.relu(self.fc1(x))\n",
    "#               x= self.dropout1(x)\n",
    "#               x = torch.relu(self.fc2(x))\n",
    "#               x=self.dropout2(x)\n",
    "#               x = torch.relu(self.fc3(x))\n",
    "#               x = self.fc4(x)\n",
    "#               return x\n",
    "\n",
    "#       # Create an instance of the EEGClassifier\n",
    "#       model = EEGClassifier()\n",
    "\n",
    "#       # Define loss function and optimizer\n",
    "#       criterion = nn.CrossEntropyLoss()\n",
    "#       optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#       # Train the feed-forward neural network\n",
    "#       num_epochs = 100\n",
    "#       for epoch in range(num_epochs):\n",
    "#           for batch_X, batch_y in train_loader:\n",
    "#               optimizer.zero_grad()\n",
    "#               outputs = model(batch_X)\n",
    "#               loss = criterion(outputs, batch_y.long())\n",
    "#               loss.backward()\n",
    "#               optimizer.step()\n",
    "\n",
    "#       # Evaluate the feed-forward neural network\n",
    "#       model.eval()\n",
    "#       with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             outputs = model(batch_X)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += batch_y.size(0)\n",
    "#             correct += (predicted == batch_y).sum().item()\n",
    "#       accuracy = correct / total\n",
    "#       test_scores.append(accuracy)\n",
    "#       print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "#     print(f\"Classification Average Test Accuracy: {np.mean(test_scores)}, Classification Average Test Std: {np.std(test_scores)}\")\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(type,subject_no, n_ele):\n",
    "    \n",
    "    X,Y=load_EEG(type, subject_no)\n",
    "    kfold=5\n",
    "    skf = StratifiedKFold(n_splits=kfold, random_state=42, shuffle=True)\n",
    "\n",
    "    test_scores=[]\n",
    "    kappa_scores=[]\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "      print(f'fold: *****************{i}*****************')\n",
    "      train_X, val_X, train_y, val_y = train_test_split(X[train_index], Y[train_index], test_size=0.10, stratify= Y[train_index], random_state=42)\n",
    "\n",
    "      test_X=X[test_index]\n",
    "      test_y=Y[test_index]\n",
    "\n",
    "\n",
    "\n",
    "      X_train, y_train  = train_augmentation(train_X, train_y)\n",
    "      X_val, y_val= train_augmentation(val_X, val_y)\n",
    "      X_test, y_test = train_augmentation(test_X, test_y)\n",
    "      y_train=y_train.reshape((-1))\n",
    "      y_val=y_val.reshape((-1))\n",
    "      y_test=y_test.reshape((-1))\n",
    "      #cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "      print(y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "      \n",
    "      cov_data_train = Covariances(estimator='lwf').transform(X_train)\n",
    "      cov_data_val=Covariances(estimator='lwf').transform(X_val)\n",
    "      cov_data_test=Covariances(estimator='lwf').transform(X_test)\n",
    "      ecs=ElectrodeSelection(nelec=n_ele, metric='riemann', n_jobs=1)\n",
    "      ecs.fit(cov_data_train, y_train)\n",
    "      cov_data_train=ecs.transform(cov_data_train)\n",
    "      cov_data_val=ecs.transform(cov_data_val)\n",
    "      cov_data_test=ecs.transform(cov_data_test)\n",
    "\n",
    "\n",
    "      ts=TangentSpace(metric='riemann', tsupdate=True)\n",
    "      data_train=ts.fit_transform(cov_data_train, y_train)\n",
    "      data_val=ts.transform(cov_data_val)\n",
    "      data_test=ts.transform(cov_data_test)\n",
    "\n",
    "      X_train_tensor = torch.Tensor(data_train)\n",
    "      y_train_tensor = torch.Tensor(y_train)\n",
    "      X_val_tensor = torch.Tensor(data_val)\n",
    "      y_val_tensor = torch.Tensor(y_val)\n",
    "      X_test_tensor = torch.Tensor(data_test)\n",
    "      y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "      # Create DataLoader for training and test sets\n",
    "      train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "      train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "      val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "      test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "      test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "      num_classes=len(words_dict[type])\n",
    "\n",
    "      # Define your feed-forward neural network\n",
    "      class EEGClassifier(nn.Module):\n",
    "          def __init__(self):\n",
    "              super(EEGClassifier, self).__init__()\n",
    "              self.fc1 = nn.Linear(in_features=data_train.shape[1], out_features=100)\n",
    "              self.dropout1 = nn.Dropout(p=0.3)\n",
    "              self.fc2 = nn.Linear(in_features=100, out_features=100)\n",
    "              self.dropout2 = nn.Dropout(p=0.3)\n",
    "              self.fc3 = nn.Linear(in_features=100, out_features=32)\n",
    "              self.fc4 = nn.Linear(in_features=32, out_features=num_classes)\n",
    "\n",
    "          def forward(self, x):\n",
    "              x = torch.relu(self.fc1(x))\n",
    "              x= self.dropout1(x)\n",
    "              x = torch.relu(self.fc2(x))\n",
    "              x=self.dropout2(x)\n",
    "              x = torch.relu(self.fc3(x))\n",
    "              x = self.fc4(x)\n",
    "              return x\n",
    "\n",
    "      # Create an instance of the EEGClassifier\n",
    "      model = EEGClassifier()\n",
    "\n",
    "      # Define loss function and optimizer\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "      # Train the feed-forward neural network\n",
    "\n",
    "\n",
    "      best_val_loss = float('inf')\n",
    "      best_model_state_dict = None\n",
    "      no_improvement_epochs = 0\n",
    "      early_stopping_epochs = 30\n",
    "\n",
    "        # Train the feed-forward neural network\n",
    "      num_epochs = 100\n",
    "      for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "                val_loss += criterion(outputs, batch_y.long()).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # Check if the current model performs better on validation set\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        # Print the validation loss and accuracy\n",
    "        if epoch % 10==0:\n",
    "          print(\"Epoch [{}/{}], Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%\"\\\n",
    "                .format(epoch+1, num_epochs, val_loss, val_accuracy * 100))\n",
    "\n",
    "        # Check for early stopping\n",
    "        if no_improvement_epochs >= early_stopping_epochs:\n",
    "            print(\"Early stopping triggered. No improvement in validation loss for {} epochs.\".format(early_stopping_epochs))\n",
    "            break\n",
    "\n",
    "        # Load the best model state dict\n",
    "      model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        # ... (rest of the code)\n",
    "\n",
    "\n",
    "      # Evaluate the feed-forward neural network\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "      \n",
    "      \n",
    "      accuracy = correct / total\n",
    "      kappa = 1-((1-accuracy)/(1-(1/num_classes)))\n",
    "      kappa_scores.append(kappa)\n",
    "      test_scores.append(accuracy)\n",
    "      print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "      print(\"Kappa: {}\".format(kappa))\n",
    "    print(f\"Classification Average Test Accuracy: {np.mean(test_scores)}, Classification Average Test Std: {np.std(test_scores)}\")\n",
    "    print(f\"Average kappa score: {np.mean(kappa_scores)}, Average kappa Std: {np.std(kappa_scores)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: *****************0*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1073, Validation Accuracy: 2.08%\n",
      "Epoch [11/100], Validation Loss: 0.6796, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6235, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6145, Validation Accuracy: 62.50%\n",
      "Epoch [41/100], Validation Loss: 0.6519, Validation Accuracy: 57.29%\n",
      "Epoch [51/100], Validation Loss: 0.7465, Validation Accuracy: 54.17%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 60.83%\n",
      "Kappa: 0.4125\n",
      "fold: *****************1*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0995, Validation Accuracy: 23.96%\n",
      "Epoch [11/100], Validation Loss: 0.7488, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6489, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6280, Validation Accuracy: 66.67%\n",
      "Epoch [41/100], Validation Loss: 0.6358, Validation Accuracy: 65.62%\n",
      "Epoch [51/100], Validation Loss: 0.7137, Validation Accuracy: 61.46%\n",
      "Epoch [61/100], Validation Loss: 0.8198, Validation Accuracy: 53.12%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 65.42%\n",
      "Kappa: 0.48125000000000007\n",
      "fold: *****************2*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0567, Validation Accuracy: 50.00%\n",
      "Epoch [11/100], Validation Loss: 0.7181, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6247, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.5585, Validation Accuracy: 67.71%\n",
      "Epoch [41/100], Validation Loss: 0.5158, Validation Accuracy: 73.96%\n",
      "Epoch [51/100], Validation Loss: 0.5249, Validation Accuracy: 70.83%\n",
      "Epoch [61/100], Validation Loss: 0.5596, Validation Accuracy: 68.75%\n",
      "Epoch [71/100], Validation Loss: 0.6111, Validation Accuracy: 69.79%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 57.92%\n",
      "Kappa: 0.36875000000000013\n",
      "fold: *****************3*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0879, Validation Accuracy: 33.33%\n",
      "Epoch [11/100], Validation Loss: 0.7057, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6288, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6096, Validation Accuracy: 67.71%\n",
      "Epoch [41/100], Validation Loss: 0.6220, Validation Accuracy: 68.75%\n",
      "Epoch [51/100], Validation Loss: 0.6635, Validation Accuracy: 72.92%\n",
      "Epoch [61/100], Validation Loss: 0.7595, Validation Accuracy: 69.79%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 56.67%\n",
      "Kappa: 0.3500000000000001\n",
      "fold: *****************4*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0974, Validation Accuracy: 34.38%\n",
      "Epoch [11/100], Validation Loss: 0.7282, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6435, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6129, Validation Accuracy: 66.67%\n",
      "Epoch [41/100], Validation Loss: 0.6060, Validation Accuracy: 66.67%\n",
      "Epoch [51/100], Validation Loss: 0.6295, Validation Accuracy: 65.62%\n",
      "Epoch [61/100], Validation Loss: 0.6817, Validation Accuracy: 64.58%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 66.67%\n",
      "Kappa: 0.5\n",
      "Classification Average Test Accuracy: 0.615, Classification Average Test Std: 0.039668767451373\n",
      "Average kappa score: 0.42250000000000004, Average kappa Std: 0.05950315117705949\n",
      "fold: *****************0*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1100, Validation Accuracy: 33.33%\n",
      "Epoch [11/100], Validation Loss: 0.7497, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6615, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.7116, Validation Accuracy: 60.42%\n",
      "Epoch [41/100], Validation Loss: 0.9114, Validation Accuracy: 59.38%\n",
      "Epoch [51/100], Validation Loss: 1.1480, Validation Accuracy: 58.33%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 65.00%\n",
      "Kappa: 0.4750000000000001\n",
      "fold: *****************1*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0454, Validation Accuracy: 33.33%\n",
      "Epoch [11/100], Validation Loss: 0.7478, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6217, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6381, Validation Accuracy: 72.92%\n",
      "Epoch [41/100], Validation Loss: 0.7719, Validation Accuracy: 68.75%\n",
      "Epoch [51/100], Validation Loss: 0.9516, Validation Accuracy: 65.62%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 62.92%\n",
      "Kappa: 0.4437500000000001\n",
      "fold: *****************2*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1492, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.7589, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6514, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.5979, Validation Accuracy: 69.79%\n",
      "Epoch [41/100], Validation Loss: 0.5987, Validation Accuracy: 71.88%\n",
      "Epoch [51/100], Validation Loss: 0.7649, Validation Accuracy: 68.75%\n",
      "Epoch [61/100], Validation Loss: 0.9114, Validation Accuracy: 67.71%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 58.33%\n",
      "Kappa: 0.3750000000000001\n",
      "fold: *****************3*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1044, Validation Accuracy: 1.04%\n",
      "Epoch [11/100], Validation Loss: 0.7081, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6322, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6383, Validation Accuracy: 63.54%\n",
      "Epoch [41/100], Validation Loss: 0.7655, Validation Accuracy: 70.83%\n",
      "Epoch [51/100], Validation Loss: 0.9385, Validation Accuracy: 67.71%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 58.33%\n",
      "Kappa: 0.3750000000000001\n",
      "fold: *****************4*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 0.9945, Validation Accuracy: 66.67%\n",
      "Epoch [11/100], Validation Loss: 0.6857, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6283, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6159, Validation Accuracy: 66.67%\n",
      "Epoch [41/100], Validation Loss: 0.6879, Validation Accuracy: 65.62%\n",
      "Epoch [51/100], Validation Loss: 0.7743, Validation Accuracy: 63.54%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 65.42%\n",
      "Kappa: 0.48125000000000007\n",
      "Classification Average Test Accuracy: 0.6200000000000001, Classification Average Test Std: 0.03111359117099077\n",
      "Average kappa score: 0.43000000000000005, Average kappa Std: 0.04667038675648616\n",
      "fold: *****************0*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0939, Validation Accuracy: 64.58%\n",
      "Epoch [11/100], Validation Loss: 0.7582, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6690, Validation Accuracy: 62.50%\n",
      "Epoch [31/100], Validation Loss: 0.8889, Validation Accuracy: 64.58%\n",
      "Epoch [41/100], Validation Loss: 1.2567, Validation Accuracy: 59.38%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 62.50%\n",
      "Kappa: 0.4375000000000001\n",
      "fold: *****************1*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0887, Validation Accuracy: 66.67%\n",
      "Epoch [11/100], Validation Loss: 0.7140, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6180, Validation Accuracy: 69.79%\n",
      "Epoch [31/100], Validation Loss: 0.6851, Validation Accuracy: 75.00%\n",
      "Epoch [41/100], Validation Loss: 0.8786, Validation Accuracy: 71.88%\n",
      "Epoch [51/100], Validation Loss: 1.1068, Validation Accuracy: 70.83%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 59.17%\n",
      "Kappa: 0.38750000000000007\n",
      "fold: *****************2*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1408, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.7755, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6520, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.5618, Validation Accuracy: 72.92%\n",
      "Epoch [41/100], Validation Loss: 0.6443, Validation Accuracy: 71.88%\n",
      "Epoch [51/100], Validation Loss: 0.7281, Validation Accuracy: 71.88%\n",
      "Epoch [61/100], Validation Loss: 0.8695, Validation Accuracy: 72.92%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 60.00%\n",
      "Kappa: 0.4\n",
      "fold: *****************3*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1400, Validation Accuracy: 33.33%\n",
      "Epoch [11/100], Validation Loss: 0.7061, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6160, Validation Accuracy: 67.71%\n",
      "Epoch [31/100], Validation Loss: 0.6374, Validation Accuracy: 68.75%\n",
      "Epoch [41/100], Validation Loss: 0.8275, Validation Accuracy: 69.79%\n",
      "Epoch [51/100], Validation Loss: 1.0358, Validation Accuracy: 70.83%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 56.67%\n",
      "Kappa: 0.3500000000000001\n",
      "fold: *****************4*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1089, Validation Accuracy: 28.12%\n",
      "Epoch [11/100], Validation Loss: 0.7425, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6625, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.6569, Validation Accuracy: 65.62%\n",
      "Epoch [41/100], Validation Loss: 0.7728, Validation Accuracy: 57.29%\n",
      "Epoch [51/100], Validation Loss: 0.9440, Validation Accuracy: 61.46%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 61.67%\n",
      "Kappa: 0.42500000000000016\n",
      "Classification Average Test Accuracy: 0.6000000000000001, Classification Average Test Std: 0.02041241452319316\n",
      "Average kappa score: 0.4000000000000001, Average kappa Std: 0.030618621784789746\n",
      "fold: *****************0*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1231, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.7051, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6706, Validation Accuracy: 64.58%\n",
      "Epoch [31/100], Validation Loss: 0.9795, Validation Accuracy: 57.29%\n",
      "Epoch [41/100], Validation Loss: 1.4387, Validation Accuracy: 57.29%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 60.42%\n",
      "Kappa: 0.40625\n",
      "fold: *****************1*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0753, Validation Accuracy: 66.67%\n",
      "Epoch [11/100], Validation Loss: 0.7143, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6506, Validation Accuracy: 66.67%\n",
      "Epoch [31/100], Validation Loss: 0.7529, Validation Accuracy: 70.83%\n",
      "Epoch [41/100], Validation Loss: 1.0195, Validation Accuracy: 65.62%\n",
      "Epoch [51/100], Validation Loss: 1.2380, Validation Accuracy: 63.54%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 60.42%\n",
      "Kappa: 0.40625\n",
      "fold: *****************2*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0862, Validation Accuracy: 66.67%\n",
      "Epoch [11/100], Validation Loss: 0.7399, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.5969, Validation Accuracy: 70.83%\n",
      "Epoch [31/100], Validation Loss: 0.7682, Validation Accuracy: 63.54%\n",
      "Epoch [41/100], Validation Loss: 0.9919, Validation Accuracy: 67.71%\n",
      "Epoch [51/100], Validation Loss: 1.2244, Validation Accuracy: 66.67%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 57.92%\n",
      "Kappa: 0.36875000000000013\n",
      "fold: *****************3*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1482, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.6852, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6130, Validation Accuracy: 64.58%\n",
      "Epoch [31/100], Validation Loss: 0.7659, Validation Accuracy: 65.62%\n",
      "Epoch [41/100], Validation Loss: 1.0214, Validation Accuracy: 66.67%\n",
      "Epoch [51/100], Validation Loss: 1.1578, Validation Accuracy: 63.54%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 60.83%\n",
      "Kappa: 0.4125\n",
      "fold: *****************4*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1215, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.7229, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6529, Validation Accuracy: 64.58%\n",
      "Epoch [31/100], Validation Loss: 0.7960, Validation Accuracy: 57.29%\n",
      "Epoch [41/100], Validation Loss: 1.0765, Validation Accuracy: 59.38%\n",
      "Epoch [51/100], Validation Loss: 1.3674, Validation Accuracy: 58.33%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 65.83%\n",
      "Kappa: 0.48750000000000004\n",
      "Classification Average Test Accuracy: 0.6108333333333333, Classification Average Test Std: 0.025900450446533413\n",
      "Average kappa score: 0.41624999999999995, Average kappa Std: 0.03885067566980012\n",
      "fold: *****************0*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1183, Validation Accuracy: 33.33%\n",
      "Epoch [11/100], Validation Loss: 0.6879, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6517, Validation Accuracy: 65.62%\n",
      "Epoch [31/100], Validation Loss: 0.9258, Validation Accuracy: 63.54%\n",
      "Epoch [41/100], Validation Loss: 1.2468, Validation Accuracy: 61.46%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 62.08%\n",
      "Kappa: 0.43125000000000013\n",
      "fold: *****************1*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0945, Validation Accuracy: 63.54%\n",
      "Epoch [11/100], Validation Loss: 0.6980, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6611, Validation Accuracy: 75.00%\n",
      "Epoch [31/100], Validation Loss: 0.8548, Validation Accuracy: 67.71%\n",
      "Epoch [41/100], Validation Loss: 1.0603, Validation Accuracy: 68.75%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 57.08%\n",
      "Kappa: 0.35625000000000007\n",
      "fold: *****************2*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.1322, Validation Accuracy: 0.00%\n",
      "Epoch [11/100], Validation Loss: 0.7252, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6032, Validation Accuracy: 68.75%\n",
      "Epoch [31/100], Validation Loss: 0.7950, Validation Accuracy: 66.67%\n",
      "Epoch [41/100], Validation Loss: 1.1032, Validation Accuracy: 66.67%\n",
      "Epoch [51/100], Validation Loss: 1.2636, Validation Accuracy: 65.62%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 61.67%\n",
      "Kappa: 0.42500000000000016\n",
      "fold: *****************3*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0788, Validation Accuracy: 75.00%\n",
      "Epoch [11/100], Validation Loss: 0.6720, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.5894, Validation Accuracy: 67.71%\n",
      "Epoch [31/100], Validation Loss: 0.6943, Validation Accuracy: 68.75%\n",
      "Epoch [41/100], Validation Loss: 0.9022, Validation Accuracy: 68.75%\n",
      "Epoch [51/100], Validation Loss: 1.0690, Validation Accuracy: 69.79%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 62.50%\n",
      "Kappa: 0.4375000000000001\n",
      "fold: *****************4*****************\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "with total_sample: 1152, epoch size: 512 and strides: 192\n",
      "(864,) (96,) (240,)\n",
      "Epoch [1/100], Validation Loss: 1.0659, Validation Accuracy: 66.67%\n",
      "Epoch [11/100], Validation Loss: 0.7073, Validation Accuracy: 66.67%\n",
      "Epoch [21/100], Validation Loss: 0.6390, Validation Accuracy: 65.62%\n",
      "Epoch [31/100], Validation Loss: 0.7891, Validation Accuracy: 66.67%\n",
      "Epoch [41/100], Validation Loss: 1.0504, Validation Accuracy: 63.54%\n",
      "Epoch [51/100], Validation Loss: 1.2444, Validation Accuracy: 64.58%\n",
      "Early stopping triggered. No improvement in validation loss for 30 epochs.\n",
      "Accuracy: 70.00%\n",
      "Kappa: 0.55\n",
      "Classification Average Test Accuracy: 0.6266666666666667, Classification Average Test Std: 0.04154983620772636\n",
      "Average kappa score: 0.44000000000000006, Average kappa Std: 0.062324754311589535\n"
     ]
    }
   ],
   "source": [
    "arr=[15,20,25,30,35]\n",
    "for i in range(len(arr)):\n",
    "    type=\"Vowels\"\n",
    "    subject_no=8\n",
    "    get_data(type, subject_no, arr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "# with open('Riemannian_Long.txt', 'a') as file:\n",
    "#     sys.stdout = file\n",
    "#     print('###########################################################################')\n",
    "#     type=\"Long_words\"\n",
    "#     subject_no=[2,6,7,9,11]\n",
    "#     for i in range(len(subject_no)):\n",
    "#         print(f'subject no: {subject_no[i]}')\n",
    "#         get_data(type, subject_no[i])\n",
    "#         print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "        \n",
    "#     sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type=\"Short_Long_words\"\n",
    "# subject_no=1\n",
    "# get_data(type, subject_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "# print('###########################################################################')\n",
    "# with open('Riemannian_Attempt.txt', 'a') as file:\n",
    "#     sys.stdout = file\n",
    "#     print('*******************************************************************************')\n",
    "#     type=\"Short_Long_words\"\n",
    "#     subject_no=[1,5,8,9,10,14]\n",
    "#     for i in range(len(subject_no)):\n",
    "#         print(f'subject no: {subject_no[i]}')\n",
    "#         get_data(type, subject_no[i])\n",
    "#         print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "\n",
    "#     sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "# print('###########################################################################')\n",
    "# with open('Riemannian_Attempt.txt', 'a') as file:\n",
    "#     sys.stdout = file\n",
    "#     print('*******************************************************************************')\n",
    "#     type=\"Short_words\"\n",
    "#     print(f'{type}')\n",
    "#     subject_no=[1,3,5,8,12]\n",
    "#     for i in range(len(subject_no)):\n",
    "#         print(f'subject no: {subject_no[i]}')\n",
    "#         get_data(type, subject_no[i])\n",
    "#         print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "\n",
    "#     sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "# with open('Riemannian_Attempt.txt', 'a') as file:\n",
    "#     sys.stdout = file\n",
    "#     print('*******************************************************************************')\n",
    "#     type=\"Vowels\"\n",
    "#     subject_no=[8,9,11,12,13,15]\n",
    "#     print(f'{type}')\n",
    "#     for i in range(len(subject_no)):\n",
    "#         print(f'subject no: {subject_no[i]}')\n",
    "#         get_data(type, subject_no[i])\n",
    "#         print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "\n",
    "#     sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
